{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fe89fd-40ac-42bd-8174-0f249af9344d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets for Gold layer\n",
    "dbutils.widgets.text(\"source_table\", \"default.silver_events\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "\n",
    "print(\"Running Gold layer\")\n",
    "print(\"Source Table:\", source_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e8e929-3b87-4fe1-9d5d-15138ca72a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "gold_df = (\n",
    "    spark.read.table(source_table)\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(\n",
    "        F.count(F.when(F.col(\"event_type\") == \"view\", True)).alias(\"views\"),\n",
    "        F.count(F.when(F.col(\"event_type\") == \"purchase\", True)).alias(\"purchases\"),\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"purchase\", F.col(\"price\"))).alias(\"revenue\")\n",
    "    )\n",
    "\n",
    "    .withColumn(\n",
    "        \"conversion_rate\",\n",
    "        F.when(F.col(\"views\") > 0, (F.col(\"purchases\") / F.col(\"views\")) * 100)\n",
    "         .otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"default.gold_product_performance\")\n",
    "\n",
    "print(\"Gold layer completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b2038a6-3cf6-47a9-b680-7c7abe10b453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"default.gold_product_performance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e864921c-0713-4b7e-a9ed-9ad4c6ae3838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.read.table(\"default.gold_product_performance\") \\\n",
    "    .filter(F.col(\"conversion_rate\").isNotNull()) \\\n",
    "    .orderBy(F.col(\"revenue\").desc()) \\\n",
    "    .limit(10) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a157c24-7a30-4a9e-ab55-2c6d2fc824ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.read.table(\"default.gold_product_performance\") \\\n",
    "    .filter(F.col(\"conversion_rate\").isNotNull()) \\\n",
    "    .orderBy(F.col(\"revenue\").desc()) \\\n",
    "    .limit(20) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa2889e-a9c9-47c1-9fe1-7ac3193156f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
    "from pyspark.sql.functions import broadcast\n",
    "top10 = (\n",
    "    spark.read.table(\"default.gold_product_performance\")\n",
    "    .filter(F.col(\"conversion_rate\").isNotNull())\n",
    "    .orderBy(F.col(\"revenue\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", LongType(), True),\n",
    "    StructField(\"category_id\", LongType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])\n",
    "product_brand = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)\n",
    "    .csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\")\n",
    "    .select(\"product_id\", \"brand\")\n",
    "    .filter(F.col(\"brand\").isNotNull())\n",
    "    .dropDuplicates([\"product_id\"])\n",
    ")\n",
    "final_output = (\n",
    "    broadcast(top10)\n",
    "    .join(product_brand, \"product_id\", \"left\")\n",
    "    .withColumn(\n",
    "        \"revenue_fmt\",\n",
    "        F.when(F.col(\"revenue\") >= 1_000_000,\n",
    "               F.concat(F.round(F.col(\"revenue\") / 1_000_000, 2), F.lit(\" M\")))\n",
    "         .when(F.col(\"revenue\") >= 1_000,\n",
    "               F.concat(F.round(F.col(\"revenue\") / 1_000, 2), F.lit(\" K\")))\n",
    "         .otherwise(F.round(F.col(\"revenue\"), 2).cast(\"string\"))\n",
    "    )\n",
    "    .orderBy(F.col(\"revenue\").desc())\n",
    "    .select(\n",
    "        \"product_id\",\n",
    "        \"brand\",\n",
    "        \"views\",\n",
    "        \"purchases\",\n",
    "        \"conversion_rate\",\n",
    "        \"revenue_fmt\"\n",
    "    )\n",
    ")\n",
    "\n",
    "final_output.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f82e172-3717-4f82-b151-87b366142bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Define schema (DO NOT infer for large CSV)\n",
    "# --------------------------------------------------\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", LongType(), True),\n",
    "    StructField(\"category_id\", LongType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"user_id\", LongType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Read 2019-Oct.csv\n",
    "# --------------------------------------------------\n",
    "\n",
    "raw_oct = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)\n",
    "    .csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\")\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Filter brand = Dell (case-insensitive)\n",
    "# --------------------------------------------------\n",
    "\n",
    "dell_df = (\n",
    "    raw_oct\n",
    "    .filter(F.lower(F.col(\"brand\")) == \"dell\")\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Create a DELTA table for Dell brand\n",
    "# --------------------------------------------------\n",
    "\n",
    "dell_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"default.dell_brand_events\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Show output\n",
    "# --------------------------------------------------\n",
    "\n",
    "spark.read.table(\"default.dell_brand_events\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc43b9da-d818-465a-99c2-417cab2bb74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Calculate views, purchases, conversion rate & revenue\n",
    "# --------------------------------------------------\n",
    "\n",
    "dell_metrics_by_product = (\n",
    "    spark.read.table(\"default.dell_brand_events\")\n",
    "    .groupBy(\"product_id\", \"brand\")\n",
    "    .agg(\n",
    "        # Views\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "        \n",
    "        # Purchases\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\"),\n",
    "        \n",
    "        # Revenue (only from purchases)\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"purchase\", F.col(\"price\")).otherwise(0)).alias(\"revenue\")\n",
    "    )\n",
    "    # Conversion rate = purchases / views\n",
    "    .withColumn(\n",
    "        \"conversion_rate\",\n",
    "        F.when(F.col(\"views\") > 0,\n",
    "               F.round((F.col(\"purchases\") / F.col(\"views\")) * 100, 2)\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Format revenue (K / M)\n",
    "# --------------------------------------------------\n",
    "\n",
    "final_output = (\n",
    "    dell_metrics_by_product\n",
    "    .withColumn(\n",
    "        \"revenue_fmt\",\n",
    "        F.when(F.col(\"revenue\") >= 1_000_000,\n",
    "               F.concat(F.round(F.col(\"revenue\") / 1_000_000, 2), F.lit(\" M\")))\n",
    "         .when(F.col(\"revenue\") >= 1_000,\n",
    "               F.concat(F.round(F.col(\"revenue\") / 1_000, 2), F.lit(\" K\")))\n",
    "         .otherwise(F.round(F.col(\"revenue\"), 2).cast(\"string\"))\n",
    "    )\n",
    "    .orderBy(F.col(\"revenue\").desc())\n",
    "    .select(\n",
    "        \"product_id\",\n",
    "        \"brand\",\n",
    "        \"views\",\n",
    "        \"purchases\",\n",
    "        \"conversion_rate\",\n",
    "        \"revenue_fmt\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Show final result\n",
    "# --------------------------------------------------\n",
    "\n",
    "final_output.show(truncate=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_layer",
   "widgets": {
    "source_table": {
     "currentValue": "default.silver_events",
     "nuid": "01753a6b-3f86-4d76-abd3-cf26ed875a8d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default.silver_events",
      "label": null,
      "name": "source_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default.silver_events",
      "label": null,
      "name": "source_table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
