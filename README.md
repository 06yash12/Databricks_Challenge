# Databricks 14 Days AI Challenge
Excited to be joining the Databricks 14 Days AI Challenge by Indian Data Club!
I’m looking forward to strengthening my fundamentals in Databricks, Apache Spark, and real‑world data engineering workflows through hands‑on learning. Excited to learn, build, and grow alongside the community over the next 14 days.

Grateful for the support and initiative by Databricks, Codebasics, and Indian Data Club.

# Day 0 | Setup & Data Loading – Databricks 14 Days AI Challenge  

Focused on setting up the environment and preparing data before starting Day 1.

What I completed:
→ Created Databricks Community Edition account and cluster
→ Configured Kaggle API credentials
→ Created schema and volume for data storage
→ Downloaded and extracted e‑commerce dataset from Kaggle
→ Loaded October & November 2019 data into Spark

Setup done. Data ready.

# Day 1 Completed (09/01/2026)– Databricks 14 Days AI Challenge 

I’ve completed Day 1 of the Databricks AI Challenge, and it was an exciting start to my journey into the Databricks Lakehouse ecosystem.

What I Learned Today:
• Why Databricks over Pandas & Hadoop
• Lakehouse Architecture fundamentals
• Overview of Databricks Workspace structure
• Real‑world industry use cases (Netflix, Shell, Comcast)

Tasks I Completed:
• Created a Databricks Community Edition account
• Explored Workspace, Compute, and Data Explorer
• Created my first Databricks notebook
• Ran basic PySpark commands

<img width="1916" height="911" alt="image" src="https://github.com/user-attachments/assets/398efe51-0881-4da1-9456-945a599cded6" />

# Day 2 Completed (10/01/2026) – Databricks 14 Days AI Challenge
I’ve completed Day 2 of the Databricks AI Challenge, where the focus was on building strong fundamentals in Apache Spark and understanding how data is processed at scale.

What I Learned Today:
• Apache Spark architecture (Driver, Executors, DAG)
• Difference between DataFrames and RDDs
• Concept of lazy evaluation in Spark
• Usage of notebook magic commands (%sql, %python, %fs)

Tasks I Completed:
• Uploaded a sample e‑commerce CSV file
• Read data into Spark DataFrames
• Performed basic operations like select, filter, groupBy, and orderBy
• Exported processed results

<img width="1293" height="911" alt="dbc-360d0a8d-aeaf cloud databricks com_editor_notebooks_1842930892741963_o=3637156226450045" src="https://github.com/user-attachments/assets/0fa7f0d5-64fa-4f3a-9b54-c451581bd012" />

<img width="1293" height="911" alt="dbc-360d0a8d-aeaf cloud databricks com_editor_notebooks_1842930892741963_o=3637156226450045 (2)" src="https://github.com/user-attachments/assets/54562d72-72d7-4394-9e88-efd3c8c5ca12" />

<img width="1917" height="906" alt="image" src="https://github.com/user-attachments/assets/a479603e-6e15-474e-9695-f9d56d2f1b9b" />

# DAY 03 Completed (11/01/2026) – Databricks 14 Days AI Challenge

I’ve completed Day 3 of the Databricks AI Challenge, and today was all about going deeper into PySpark transformations and real-world data processing.

What I Learned Today:
• Key differences between PySpark and Pandas for large-scale data processing
• Implementing joins (inner, left, right, full outer) in PySpark
• Using window functions for running totals and ranking analytics
• Creating and applying User-Defined Functions (UDFs)

Tasks I Completed:
• Loaded the full e-commerce dataset into Databricks
• Performed complex joins across multiple tables
• Calculated running totals using window functions
• Built derived features for analytical use cases

<img width="1919" height="911" alt="image" src="https://github.com/user-attachments/assets/05310af5-a450-4b35-91b8-5adc7f79082c" />

<img width="1919" height="911" alt="image" src="https://github.com/user-attachments/assets/8aa58e9f-a09c-44ab-b189-7c80a5147e95" />

<img width="1919" height="913" alt="image" src="https://github.com/user-attachments/assets/e7451d2f-6b27-44d6-9f3b-ca0dbb1cee4b" />

# DAY 04 Completed (12/01/2026) – Databricks 14 Days AI Challenge

I’ve completed Day 4 of the Databricks AI Challenge, focusing on Delta Lake fundamentals and reliability in data engineering.

What I Learned Today:
• Understanding Delta Lake architecture and how it extends Parquet
• How ACID transactions ensure data consistency in distributed systems
• Schema enforcement vs schema evolution in Delta tables
• Key differences between Delta Lake and Parquet for analytics workloads

Tasks I Completed:
• Converted raw CSV data into Delta format
• Created managed Delta tables using SQL and PySpark
• Tested schema enforcement by attempting invalid writes
• Handled duplicate inserts using Delta Lake constraints and overwrite strategies

<img width="1919" height="910" alt="image" src="https://github.com/user-attachments/assets/5d00833f-964c-4b71-9625-79aac166a764" />

<img width="1919" height="911" alt="image" src="https://github.com/user-attachments/assets/79a10663-afaa-4acc-bdfe-0055d801c372" />




