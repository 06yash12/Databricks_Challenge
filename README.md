<img width="1580" height="300" alt="banner" src="https://github.com/user-attachments/assets/54b3b592-3e41-4af9-a404-e4bf57faf2eb" />


<div align="center">

# Databricks 14-Days of AI Challenge

</div>

Excited to be joining the Databricks 14 Days AI Challenge by Indian Data Club!
I’m looking forward to strengthening my fundamentals in Databricks, Apache Spark, and real‑world data engineering workflows through hands‑on learning. Excited to learn, build, and grow alongside the community over the next 14 days.

Grateful for the support and initiative by [Databricks,](https://databricks.com/) [Codebasics](https://codebasics.io/) and [Indian Data Club.](https://www.linkedin.com/company/indian-data-club/)

# Day 0 (08/01/26)– Setup & Data Loading  

Focused on setting up the environment and preparing data before starting Day 1.

What I completed:
→ Created Databricks Community Edition account and cluster
→ Configured Kaggle API credentials
→ Created schema and volume for data storage
→ Downloaded and extracted e‑commerce dataset from Kaggle
→ Loaded October & November 2019 data into Spark

Setup done. Data ready.

# PHASE 1: FOUNDATION (Days 1-4)

# DAY 1 (09/01/26)– Platform Setup & First Steps

I’ve completed Day 1 of the Databricks AI Challenge, and it was an exciting start to my journey into the Databricks Lakehouse ecosystem.

What I Learned Today:
• Why Databricks over Pandas & Hadoop
• Lakehouse Architecture fundamentals
• Overview of Databricks Workspace structure
• Real‑world industry use cases (Netflix, Shell, Comcast)

Tasks I Completed:
• Created a Databricks Community Edition account
• Explored Workspace, Compute, and Data Explorer
• Created my first Databricks notebook
• Ran basic PySpark commands

<img width="1916" height="911" alt="image" src="https://github.com/user-attachments/assets/398efe51-0881-4da1-9456-945a599cded6" />

# DAY 2 (10/01/26) – Apache Spark Fundamentals
I’ve completed Day 2 of the Databricks AI Challenge, where the focus was on building strong fundamentals in Apache Spark and understanding how data is processed at scale.

What I Learned Today:
• Apache Spark architecture (Driver, Executors, DAG)
• Difference between DataFrames and RDDs
• Concept of lazy evaluation in Spark
• Usage of notebook magic commands (%sql, %python, %fs)

Tasks I Completed:
• Uploaded a sample e‑commerce CSV file
• Read data into Spark DataFrames
• Performed basic operations like select, filter, groupBy, and orderBy
• Exported processed results

<img width="1293" height="911" alt="dbc-360d0a8d-aeaf cloud databricks com_editor_notebooks_1842930892741963_o=3637156226450045" src="https://github.com/user-attachments/assets/0fa7f0d5-64fa-4f3a-9b54-c451581bd012" />

<img width="1293" height="911" alt="dbc-360d0a8d-aeaf cloud databricks com_editor_notebooks_1842930892741963_o=3637156226450045 (2)" src="https://github.com/user-attachments/assets/54562d72-72d7-4394-9e88-efd3c8c5ca12" />

<img width="1917" height="906" alt="image" src="https://github.com/user-attachments/assets/a479603e-6e15-474e-9695-f9d56d2f1b9b" />

# DAY 3 (11/01/26) – PySpark Transformations Deep Dive

I’ve completed Day 3 of the Databricks AI Challenge, and today was all about going deeper into PySpark transformations and real-world data processing.

What I Learned Today:
• Key differences between PySpark and Pandas for large-scale data processing
• Implementing joins (inner, left, right, full outer) in PySpark
• Using window functions for running totals and ranking analytics
• Creating and applying User-Defined Functions (UDFs)

Tasks I Completed:
• Loaded the full e-commerce dataset into Databricks
• Performed complex joins across multiple tables
• Calculated running totals using window functions
• Built derived features for analytical use cases

<img width="1919" height="911" alt="image" src="https://github.com/user-attachments/assets/05310af5-a450-4b35-91b8-5adc7f79082c" />

<img width="1919" height="911" alt="image" src="https://github.com/user-attachments/assets/8aa58e9f-a09c-44ab-b189-7c80a5147e95" />

<img width="1919" height="913" alt="image" src="https://github.com/user-attachments/assets/e7451d2f-6b27-44d6-9f3b-ca0dbb1cee4b" />

# DAY 4 (12/01/26)– Delta Lake Introduction

I’ve completed Day 4 of the Databricks AI Challenge, focusing on Delta Lake fundamentals and reliability in data engineering.

What I Learned Today:
• Understanding Delta Lake architecture and how it extends Parquet
• How ACID transactions ensure data consistency in distributed systems
• Schema enforcement vs schema evolution in Delta tables
• Key differences between Delta Lake and Parquet for analytics workloads

Tasks I Completed:
• Converted raw CSV data into Delta format
• Created managed Delta tables using SQL and PySpark
• Tested schema enforcement by attempting invalid writes
• Handled duplicate inserts using Delta Lake constraints and overwrite strategies

<img width="1919" height="910" alt="image" src="https://github.com/user-attachments/assets/5d00833f-964c-4b71-9625-79aac166a764" />

<img width="1919" height="911" alt="image" src="https://github.com/user-attachments/assets/79a10663-afaa-4acc-bdfe-0055d801c372" />

# PHASE 2: DATA ENGINEERING (Days 5-8)
# DAY 5 (13/01/26) – Delta Lake Advanced
I’ve successfully completed Day 5 of the Databricks AI Challenge, diving deep into advanced Delta Lake operations that enable reliable, high-performance data engineering workflows.

What I Learned Today:
• Delta Lake time travel and version history querying
• Incremental data processing using MERGE (upserts)
• Performance tuning with OPTIMIZE and Z-ORDER
• Data lifecycle management using VACUUM

Tasks I Completed:
• Implemented incremental MERGE operations on Delta tables
• Queried historical versions using time travel
• Optimized Delta tables for faster query performance
• Cleaned up obsolete data files using VACUUM

<img width="1913" height="908" alt="image" src="https://github.com/user-attachments/assets/1fd747e8-687d-443e-ba4e-96f0248156cd" />

<img width="1919" height="909" alt="image" src="https://github.com/user-attachments/assets/77cc612a-2a4b-428a-8533-b7e8252b414c" />

<img width="1919" height="909" alt="image" src="https://github.com/user-attachments/assets/d4830453-e8a1-4684-9c3d-a42f7f086b4c" />

<img width="1919" height="898" alt="image" src="https://github.com/user-attachments/assets/436d17b5-f94e-4e0c-ae4b-e5cb8dd9c9e8" />


# DAY 6 (14/01/26) – Medallion Architecture

I’ve successfully completed Day 6 of the Databricks AI Challenge, focusing on building a scalable Medallion Architecture to structure data pipelines for analytics-ready workloads.

What I Learned Today:
• Medallion Architecture: Bronze (raw) → Silver (cleaned) → Gold (aggregated)
• Best practices for designing each data layer
• Incremental processing patterns for efficient data movement

Tasks I Completed:
• Designed a robust 3-layer Medallion architecture
• Built the Bronze layer for raw data ingestion
• Implemented the Silver layer with data cleaning and validation
• Created the Gold layer with business-ready aggregations

<img width="1919" height="914" alt="image" src="https://github.com/user-attachments/assets/d97193af-5477-4ad1-bb7c-46c0612a59c3" />

<img width="1919" height="914" alt="image" src="https://github.com/user-attachments/assets/b68b2ce3-1876-431a-9696-7b238d303e4b" />

# DAY 7 (15/01/26) – Workflows & Job Orchestration

I’ve successfully completed Day 7 of the Databricks AI Challenge, focusing on orchestrating production-grade data pipelines using Databricks Jobs.

What I Learned Today:
• Differences between Databricks Jobs and notebooks
• Building multi-task workflows for end-to-end pipelines
• Using parameters and scheduling for automation
• Implementing error handling and retries

Tasks I Completed:
• Added parameter widgets to notebooks
• Created a multi-task job workflow (Bronze → Silver → Gold)
• Configured task dependencies
• Scheduled automated job execution

<img width="1919" height="914" alt="image" src="https://github.com/user-attachments/assets/e8d4ef4b-5f17-4567-8c6c-420017623f29" />

<img width="1919" height="911" alt="image" src="https://github.com/user-attachments/assets/9b50ca09-890c-4331-a702-ae7dfb45ecb1" />

<img width="1919" height="914" alt="image" src="https://github.com/user-attachments/assets/7f0280f7-53cd-4dee-a73e-d309384d3c92" />

<img width="1919" height="908" alt="image" src="https://github.com/user-attachments/assets/4cfd0efd-ff6d-4f94-822a-3cf9675d4cd4" />

<img width="1919" height="913" alt="image" src="https://github.com/user-attachments/assets/e0a7691c-2b7d-4a74-a031-68c26ac1ec07" />


# DAY 8 (16/01/26) – Unity Catalog Governance

I’ve successfully completed Day 8 of the Databricks AI Challenge, where the focus was on data governance using Unity Catalog—a critical foundation for secure, scalable, and enterprise-grade data platforms in Databricks.

What I Learned Today:
• Catalog → Schema → Table hierarchy in Unity Catalog
• Fine-grained access control using GRANT / REVOKE
• Understanding and exploring data lineage
• Differences between managed and external tables

Tasks I Completed:
• Created catalogs and schemas in Unity Catalog
• Registered Delta tables under governed catalogs
• Configured permissions for secure data access
• Built views to enable controlled, role-based data consumption

<img width="1919" height="911" alt="image" src="https://github.com/user-attachments/assets/26a41678-bcf0-485e-974c-2bd633a8af6b" />

<img width="1919" height="915" alt="image" src="https://github.com/user-attachments/assets/5c7502c6-1396-4a67-9379-a712f56c149c" />

<img width="1919" height="910" alt="image" src="https://github.com/user-attachments/assets/719340b9-0b05-4568-babd-3b6bfe88fb98" />

# PHASE 3: ADVANCED ANALYTICS (Days 9-11)
# DAY 9 (17/01/26) – SQL Analytics & Dashboards
I’ve successfully completed Day 9 of the Databricks AI Challenge, focusing on SQL Analytics and Dashboarding to transform curated data into actionable business insights using Databricks SQL.

What I Learned Today:
• SQL Warehouses and their role in scalable analytics
• Writing complex analytical SQL queries
• Building interactive dashboards
• Creating effective visualizations and filters
• Scheduling automated dashboard refreshes

Tasks I Completed:
• Created and configured a SQL Warehouse
• Wrote analytical SQL queries for insights
• Built dashboards covering
• Revenue trends
• Conversion funnels
• Top-performing products
• Added interactive filters and scheduled auto-refresh

<img width="1919" height="898" alt="image" src="https://github.com/user-attachments/assets/27f503d2-e1e4-415e-811d-efd482063957" />

<img width="1919" height="896" alt="image" src="https://github.com/user-attachments/assets/9ab7e7cd-db13-4d5d-a690-0a24ac41b5ef" />

<img width="1919" height="910" alt="image" src="https://github.com/user-attachments/assets/52b978dd-63ba-4af6-b677-3f4d9d68a594" />

<img width="1919" height="916" alt="image" src="https://github.com/user-attachments/assets/22551bda-cf23-4524-9e7e-ebf2de9a846e" />

<img width="1919" height="915" alt="image" src="https://github.com/user-attachments/assets/1cd4755e-6826-4ff4-8818-d40f651741d3" />

<img width="1915" height="906" alt="image" src="https://github.com/user-attachments/assets/fe979ccb-1210-45df-9187-a81b7e2c5c21" />

<img width="1919" height="900" alt="image" src="https://github.com/user-attachments/assets/d429d1b1-ba4b-4feb-86b0-e22cbbf0cb86" />

<img width="1919" height="906" alt="image" src="https://github.com/user-attachments/assets/faf90fab-eb2c-41dd-a0da-ce5e44848241" />

# DAY 10 (18/01/26) – Performance Optimization
I’ve successfully completed Day 10 of the Databricks AI Challenge, focused on query performance optimization—an essential step for building fast, scalable analytics on large datasets.

What I Learned Today:
• Understanding and interpreting Spark SQL query execution plans
• Effective partitioning strategies for large Delta tables
• Performance tuning using OPTIMIZE and ZORDER
• Leveraging caching techniques to reduce query latency

Tasks I Completed:
• Analyzed query execution plans to identify bottlenecks
• Partitioned large tables to improve scan efficiency
• Applied OPTIMIZE and ZORDER for data layout optimization
• Benchmarked performance improvements before and after tuning

<img width="1919" height="912" alt="image" src="https://github.com/user-attachments/assets/fbd6e041-56ff-4354-b92b-aea48197f043" />

<img width="1919" height="911" alt="image" src="https://github.com/user-attachments/assets/49070167-20ff-4f00-8e6c-344d66a5ffc6" />

# DAY 11 (19/01/26) – Statistical Analysis & ML Prep

I’ve successfully completed Day 11 of the Databricks AI Challenge, focused on statistical analysis and preparing data for machine learning workflows.

What I Learned Today:
• Descriptive statistics for understanding data distributions
• Hypothesis testing concepts and significance testing
• A/B test design and evaluation
• Feature engineering techniques for ML models

Tasks I Completed:
• Calculated statistical summaries to analyze key metrics
• Tested hypotheses comparing weekday vs weekend behavior
• Identified correlations among numerical features
• Engineered features suitable for machine learning models

<img width="1919" height="906" alt="image" src="https://github.com/user-attachments/assets/c1ccac95-fcfa-4eca-8e9a-c7721d3df83c" />

<img width="1919" height="899" alt="image" src="https://github.com/user-attachments/assets/f7eb6486-8d3d-4200-bcd3-de275b68c8f3" />

<img width="1916" height="915" alt="image" src="https://github.com/user-attachments/assets/eab53c87-5426-414a-a677-e28182b483c8" />

<img width="1919" height="910" alt="image" src="https://github.com/user-attachments/assets/af2cf49d-6649-4e3e-92d3-4f89713ebcec" />

# PHASE 4: AI & ML (Days 12-14)
# DAY 12 (20/01/26) – MLflow Basics

I successfully completed Day 12 of the Databricks AI Challenge, focused on MLflow fundamentals and experiment tracking.

What I Learned Today:
• MLflow components (tracking, models, registry)
• Experiment tracking concepts
• Logging parameters, metrics, and models
• Using MLflow UI to analyze runs
• Comparing multiple model runs

Tasks I Completed:
• Trained simple regression models
• Logged parameters, metrics, and models to MLflow
• Viewed experiment runs in MLflow UI
• Compared model performance across runs

<img width="1919" height="912" alt="image" src="https://github.com/user-attachments/assets/b6e5a3ee-a02d-4890-a76c-adbe1472bfd0" />

<img width="1919" height="915" alt="image" src="https://github.com/user-attachments/assets/bc86d95f-abe1-4d5a-97ae-920952106419" />

<img width="1918" height="909" alt="image" src="https://github.com/user-attachments/assets/7e43a517-6aae-435f-89ef-ebef103a5018" />


# DAY 13 (21/01/26) – Model Comparison & Feature Engineering

I successfully completed Day 13 of the Databricks AI Challenge, focused on training multiple models, comparing their performance, and working with Spark ML pipelines.

What I Learned Today:
• Training and evaluating multiple machine learning models
• Comparing model metrics using MLflow
• Understanding model performance differences
• Building and training Spark ML Pipelines
• Selecting the best model based on evaluation metrics

Tasks I Completed:
• Trained three regression models (Linear Regression, Decision Tree, Random Forest)
• Logged and compared model metrics in MLflow
• Built a Spark ML pipeline using VectorAssembler and Linear Regression
• Evaluated Spark ML model performance
• Selected the best-performing model based on R² score

<img width="1919" height="912" alt="image" src="https://github.com/user-attachments/assets/c5776547-b5da-439f-963c-604ba3b4a110" />
<img width="1919" height="898" alt="image" src="https://github.com/user-attachments/assets/4e5610ce-bdc6-4611-8109-e91e71bf6e05" />
<img width="1919" height="909" alt="image" src="https://github.com/user-attachments/assets/1f64069e-d0c8-402f-8a16-b25edc6d3651" />
<img width="1919" height="864" alt="image" src="https://github.com/user-attachments/assets/82114020-250c-44c3-a3d3-bcf68576ed6d" />
<img width="1919" height="905" alt="image" src="https://github.com/user-attachments/assets/a6970511-9ced-4e61-95d7-e87d410236c7" />



































